

\newpage
\section{Introduzione}
L'anomaly detection su immagini è un compito particolarmente complesso, poiché richiede
non solo l'identificazione di osservazioni che deviano dal comportamento normale, ma
anche la capacità di spiegare quali componenti dell'immagine giustifichino tale devianza.
In numerosi contesti applicativi --- come ispezione industriale, manutenzione predittiva o
monitoraggio di qualità --- non è sufficiente stabilire se un'immagine sia anomala: è
necessario produrre una spiegazione interpretabile e coerente che evidenzi le regioni
responsabili dell'anomalia.

La letteratura tradizionale sull'Explainable Artificial Intelligence (xAI) si è concentrata
principalmente su modelli di classificazione o regressione, mentre una minore attenzione è
stata dedicata al problema dell'Explainable Anomaly Detection (xAD). Tuttavia, le
peculiarità del rilevamento di anomalie rendono i metodi post-hoc scarsamente efficaci in
questo scenario \cite{AE_XAD_Arrays}. In particolare, tre criticità strutturali ostacolano
l'applicazione diretta delle tecniche di spiegabilità classiche:
\begin{itemize}
    \item \textbf{Rarità delle anomalie:} i difetti sono poco rappresentati e non esiste un
    processo affidabile per generarli artificialmente in modo realistico; ciò limita l'uso
    di metodi post-hoc basati sull'abbondanza di esempi anomali.
    \item \textbf{Eterogeneità delle anomalie:} forme, estensioni e intensità possono variare
    significativamente rendendo difficile definire un modello universale.
    \item \textbf{Bassa fedeltà delle spiegazioni post-hoc:} tali metodi operano su modelli
    già addestrati, non progettati per essere esplicabili, producendo mappe che spesso non
    riflettono accuratamente le regioni realmente responsabili dell'anomalia.
\end{itemize}

A fronte di questi limiti, è cresciuto l'interesse verso approcci \textit{explainability-by-design},
ossia modelli che integrano il meccanismo di spiegazione all'interno del processo
di addestramento. In questo contesto si colloca \textbf{AE--XAD}, un metodo basato su
Autoencoder che ha come obiettivo non solo il rilevamento dell'anomalia, ma anche la
produzione di una \textit{heatmap} interpretabile in grado di evidenziare le regioni che più
contribuiscono al comportamento anomalo.

Il problema affrontato da AE--XAD può essere formulato come segue:
\begin{quote}
    \textit{Data un'immagine potenzialmente anomala, determinare non solo se essa contenga
    difetti, ma anche quali regioni dell'immagine siano responsabili della deviazione rispetto
    al comportamento normale.}
\end{quote}

I metodi ricostruttivi convenzionali, come gli Autoencoder standard, non sono sufficienti
per questo scopo: tendono infatti a ricostruire anche le regioni anomale e a produrre mappe
di errore poco informative e rumorose. AE--XAD supera tali limitazioni introducendo una
strategia di addestramento semi-supervisionata progettata per:
\begin{itemize}
    \item ricostruire accuratamente i pixel normali;
    \item ricostruire intenzionalmente \emph{in modo errato} i pixel anomali;
    \item generare una heatmap stabile, coerente e interpretabile basata sull'errore di ricostruzione.
\end{itemize}

Nelle sezioni successive si analizzeranno i principi operativi del metodo, la formulazione
matematica della loss, la struttura dell'architettura e il processo di generazione della heatmap.